{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef4d6206",
   "metadata": {},
   "source": [
    "# Selecting nice images generated by Stable Diffusion using the CLIP Score\n",
    "This notebook shows a process to **select the best images generated by stable diffusion** in a text to image setting. There are two potential settings where this might be useful:\n",
    "1. You have a prompt dataset and just want to **explore** the most promising images and prompts.\n",
    "2. You have a concrete task and want to find out **which possible promps** yield the best results for your task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d721da",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ff8981",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U sliceguard diffusers[torch] datasets invisible_watermark transformers accelerate safetensors torchmetrics Pillow kaleido"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "969cf840",
   "metadata": {},
   "source": [
    "# Step 1: Generate Stable Diffusion Images\n",
    "This step is simply **generating images** from text using stable diffusion. We use a prompt dataset on the huggingface hub for that. Later we want to filter especially nice images using the CLIP Score metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e07f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some imports you need\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import uuid\n",
    "from diffusers import DiffusionPipeline\n",
    "import torch\n",
    "import datasets\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ada6deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I just chose the currently most trending prompt dataset on the huggingface hub.\n",
    "# Replace that with anything that suits your need better or potentially your own\n",
    "# list of potential prompts.\n",
    "prompt_dataset = datasets.load_dataset(\"Gustavosta/Stable-Diffusion-Prompts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44dd5334",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the stable diffusion model as well as the refiner.\n",
    "pipe = DiffusionPipeline.from_pretrained(\"stabilityai/stable-diffusion-xl-base-0.9\", torch_dtype=torch.float16)\n",
    "pipe.enable_model_cpu_offload()\n",
    "\n",
    "rf_pipe = DiffusionPipeline.from_pretrained(\"stabilityai/stable-diffusion-xl-refiner-0.9\", torch_dtype=torch.float16, use_safetensors=True, variant=\"fp16\")\n",
    "rf_pipe.enable_model_cpu_offload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b76af86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a bunch of images in the directory \"images\"\n",
    "# The prompt dataset is relatively large so it could make sense to stop early.\n",
    "target_dir = Path(\"images\")\n",
    "if not target_dir.is_dir():\n",
    "    target_dir.mkdir()\n",
    "else:\n",
    "    shutil.rmtree(target_dir)\n",
    "    target_dir.mkdir()\n",
    "\n",
    "prompts = []\n",
    "generated_images = []\n",
    "for prompt in prompt_dataset[\"train\"]:\n",
    "    try:\n",
    "        prompt = prompt[\"Prompt\"]\n",
    "        image = pipe(prompt, output_type=\"latent\").images\n",
    "\n",
    "        image = rf_pipe(prompt=prompt, image=image).images[0]\n",
    "\n",
    "        image_name = f\"{str(uuid.uuid4())}.png\"\n",
    "        image_path = target_dir / image_name\n",
    "        image.save(image_path)\n",
    "        prompts.append(prompt)\n",
    "        generated_images.append(str(image_path))\n",
    "\n",
    "        df = pd.DataFrame(data={\"image\": generated_images, \"prompt\": prompts})\n",
    "        df.to_json(\"sd_dataset.json\", orient=\"records\") # save this after every generation to not loose progress in case of crashing\n",
    "    except:\n",
    "        print(\"An error occured while generating image.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd03908",
   "metadata": {},
   "source": [
    "# Step 2: Generate CLIP Scores for all the examples\n",
    "The CLIP Score is basically a **correlation between a text and the contents of an image**. It supposedly is highly correlated with human judgement and could thus be used to **filter for promising image generations**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e0c1875",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some imports you need for this step\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torchmetrics.multimodal.clip_score import CLIPScore\n",
    "import PIL\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9888184a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the CLIP Score metric\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "metric = CLIPScore(model_name_or_path=\"openai/clip-vit-large-patch14\") # openai/clip-vit-base-patch16\n",
    "metric = metric.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3482caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the dataset generated in the previous step\n",
    "df = pd.read_json(\"sd_dataset.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc59c783",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the clip scores for each image\n",
    "clip_scores = []\n",
    "for idx, row in tqdm(df.iterrows(), total=len(df)):\n",
    "    try:\n",
    "        img = PIL.Image.open(row[\"image\"])\n",
    "        img = img.convert('RGB')\n",
    "        np_img = np.array(img)\n",
    "        clip_score = metric(torch.Tensor(np_img).to(device), row[\"prompt\"]).detach().cpu().numpy()\n",
    "        img.close()\n",
    "        clip_scores.append(clip_score)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        clip_scores.append(np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60830a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the clip scores in a new dataset, removing prompts that were too long for the metric\n",
    "scored_df = pd.concat((df, pd.DataFrame(data={\"clip_score\": clip_scores})), axis=1)\n",
    "scored_df = scored_df.dropna()\n",
    "scored_df[\"prompt\"] = scored_df[\"prompt\"].astype(\"str\")\n",
    "scored_df.to_json(\"sd_dataset_scored.json\", orient=\"records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e68919f",
   "metadata": {},
   "source": [
    "# Step 3: Precompute CLIP embeddings for the image text pairs\n",
    "We want to identify clusters of images/prompts that are especially well scored (CLIP score). Therefore we have to generate a **text and image representation to cluster on**. We simply also precompute the **CLIP embeddings** for text and images and add them to the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468b57ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some imports you need\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ecaac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_json(\"sd_dataset_scored.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb5eba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the CLIP model from the huggingface hub\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-large-patch14\", output_hidden_states=True).to(device)\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-large-patch14\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d00405",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate clip embeddings for images and texts\n",
    "clip_image_embeddings = []\n",
    "clip_text_embeddings = []\n",
    "for idx, row in tqdm(df.iterrows(), total=len(df)):\n",
    "    with Image.open(row[\"image\"]) as img:\n",
    "        inputs = processor(text=[row[\"prompt\"]], images=[img], return_tensors=\"pt\", padding=True).to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            image_embedding = outputs[\"image_embeds\"].detach().cpu().numpy()[0]\n",
    "            text_embedding = outputs[\"text_embeds\"].detach().cpu().numpy()[0]\n",
    "    clip_image_embeddings.append(image_embedding)\n",
    "    clip_text_embeddings.append(text_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51782861",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the new dataset\n",
    "df[\"clip_text_embedding\"] = [e.tolist() for e in clip_text_embeddingsxt_embeddings]\n",
    "df[\"clip_image_embedding\"] = [e.tolist() for e in clip_image_embeddings]\n",
    "df.to_json(\"sd_dataset_scored_embedded.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd354bc",
   "metadata": {},
   "source": [
    "# Step 4: Selection of especially appealing images according to CLIP\n",
    "We now want to select the most appealing images according to clip score. We here explore two strategies:\n",
    "1. **Globally** select those clusters with higher than average CLIP score.\n",
    "2. First detect larger clusters in the data in which we then search for promising images **cluster-by-cluster**.\n",
    "\n",
    "Note: More on why the second strategy makes sense later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c2af989",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some imports you need\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sliceguard import SliceGuard\n",
    "from renumics import spotlight\n",
    "from renumics.spotlight import Image, Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fadcb66e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_json(\"sd_dataset_scored_embedded.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b7bf9112",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the text and image embeddings from the dataframe\n",
    "clip_text_embeddings = np.vstack(df[\"clip_text_embedding\"])\n",
    "clip_image_embeddings = np.vstack(df[\"clip_image_embedding\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186f8b27",
   "metadata": {},
   "source": [
    "## Global selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "551ac2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a metric that simply returns the precomputed metric\n",
    "def return_precomputed_metric(y, y_pred):\n",
    "    return y.mean(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251d3ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let sliceguard search for clusters in the image embedding that at least contain 2 images\n",
    "# and that have a CLIP score that is at least 4.5 OVER the average CLIP score of the whole dataset.\n",
    "sg = SliceGuard()\n",
    "sg.find_issues(df, [\"clip_image_embedding\"],\n",
    "               \"clip_score\",\n",
    "               \"clip_score\",\n",
    "               return_precomputed_metric,\n",
    "               metric_mode=\"min\",\n",
    "               min_support=2,\n",
    "               min_drop=4.5,\n",
    "              precomputed_embeddings={\"clip_image_embedding\": clip_image_embeddings})\n",
    "\n",
    "# Note: There is no explicit interface for using a precomputed metric, however just supply the metric column\n",
    "# for y and y_pred and return y mean in your metric function\n",
    "\n",
    "# Note 2: Metric mode is set to \"min\" here. That is because we want to search for especially good images.\n",
    "# Normally the natural thing for the metric would be to set it to \"max\" and get images that are especially bad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c714c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the results in renumics Spotlight\n",
    "sg.report(spotlight_dtype={\"image\": Image, \"clip_image_embedding\": Embedding})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca8d713",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can do the same thing using the text embeddings for clustering.\n",
    "sg = SliceGuard()\n",
    "sg.find_issues(df, [\"clip_text_embedding\"],\n",
    "               \"clip_score\",\n",
    "               \"clip_score\",\n",
    "               return_precomputed_metric,\n",
    "               metric_mode=\"min\",\n",
    "               min_support=2,\n",
    "               min_drop=4.5,\n",
    "              precomputed_embeddings={\"clip_text_embedding\": clip_text_embeddings})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08bae0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the results in renumics Spotlight\n",
    "sg.report(spotlight_dtype={\"image\": Image, \"clip_text_embedding\": Embedding})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf4334e",
   "metadata": {},
   "source": [
    "**So, does this work well? No! The CLIP Score metric is extremely biased towards people portraits and especially well known concepts like the faces of prominent personalities.**\n",
    "\n",
    "**What can we do about it? We can probably first compute clusters in the data to detect some sort of \"categories\" and then apply the search for significantly better clusters for each category (could be such categories as landscapes, people portraits, things, ...)**\n",
    "\n",
    "**We implemented that below!**\n",
    "\n",
    "**Note also, that the selection based on text embeddings seems to yield slightly more consistent results in our case. Meaning, the clusters are less prone to contain outliers in the CLIP score metric** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cadd256",
   "metadata": {},
   "source": [
    "## Category-wise, adaptive selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bed23290",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform this only on one type of embedding which you can select here\n",
    "EMBEDDING_TYPE = \"clip_image_embeddings\" # clip_text_embeddings\n",
    "if EMBEDDING_TYPE == \"clip_image_embeddings\":\n",
    "    embeddings = clip_image_embeddings\n",
    "elif EMBEDDING_TYPE == \"clip_text_embeddings\":\n",
    "    embeddings = clip_text_embeddings\n",
    "else:\n",
    "    raise RuntimeError(\"No valid choice for embedding type.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ef9a9c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# An additional import. Note that you have to run the above section as well.\n",
    "from hnne import HNNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "71e50b16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  3  13 160]\n"
     ]
    }
   ],
   "source": [
    "# Detect clusters in the data. Note that here the granularity is chosen by findind the first clustering that\n",
    "# contains over 3 and less than 25 clusters. If that does not apply to your data you will get an error.\n",
    "# Just shift the limits then.\n",
    "hnne = HNNE(metric=\"euclidean\")\n",
    "projection = hnne.fit_transform(embeddings)\n",
    "df[\"projection_x\"] = projection[:, 0]\n",
    "df[\"projection_y\"] = projection[:, 1]\n",
    "partitions = hnne.hierarchy_parameters.partitions\n",
    "partitions = np.flip(partitions, axis=1) \n",
    "partition_sizes = np.flip(np.array(hnne.hierarchy_parameters.partition_sizes))\n",
    "print(partition_sizes)\n",
    "for partition_idx in range(partitions.shape[1]):\n",
    "    df[f\"clustering_{partition_idx}\"] = partitions[:, partition_idx]\n",
    "chosen_partition_level = None\n",
    "for partition_level, partition_size in enumerate(partition_sizes):\n",
    "    if partition_size > 3 and partition_size < 25:\n",
    "        chosen_partition_level = partition_level\n",
    "        break\n",
    "assert chosen_partition_level is not None\n",
    "clustering_partition = partitions[:, chosen_partition_level]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3d7b5aee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The overall metric value is 29.57892399299634\n",
      "Using 2 as minimum support for determining problematic clusters.\n",
      "Using 3.7496109339632384 as minimum drop for determining problematic clusters.\n",
      "Identified 2 problematic slices.\n",
      "The overall metric value is 30.998585888159816\n",
      "Using 2 as minimum support for determining problematic clusters.\n",
      "Using 3.659101791567243 as minimum drop for determining problematic clusters.\n",
      "Identified 1 problematic slices.\n",
      "The overall metric value is 31.632761600405814\n",
      "Using 2 as minimum support for determining problematic clusters.\n",
      "Using 3.702847682845617 as minimum drop for determining problematic clusters.\n",
      "Identified 6 problematic slices.\n",
      "The overall metric value is 30.551769518976187\n",
      "Using 2 as minimum support for determining problematic clusters.\n",
      "Using 3.6821321892904457 as minimum drop for determining problematic clusters.\n",
      "Identified 3 problematic slices.\n",
      "The overall metric value is 30.245432414699994\n",
      "Using 2 as minimum support for determining problematic clusters.\n",
      "Using 3.6066276691164023 as minimum drop for determining problematic clusters.\n",
      "Identified 0 problematic slices.\n",
      "The overall metric value is 31.936802189\n",
      "Using 2 as minimum support for determining problematic clusters.\n",
      "Using 3.269144865397427 as minimum drop for determining problematic clusters.\n",
      "Identified 1 problematic slices.\n",
      "The overall metric value is 30.474840354915\n",
      "Using 2 as minimum support for determining problematic clusters.\n",
      "Using 3.9458786519521976 as minimum drop for determining problematic clusters.\n",
      "Identified 0 problematic slices.\n",
      "The overall metric value is 32.05333910317242\n",
      "Using 2 as minimum support for determining problematic clusters.\n",
      "Using 4.100120843499734 as minimum drop for determining problematic clusters.\n",
      "Identified 1 problematic slices.\n",
      "The overall metric value is 27.797508557633336\n",
      "Using 2 as minimum support for determining problematic clusters.\n",
      "Using 3.3663237747582775 as minimum drop for determining problematic clusters.\n",
      "Identified 1 problematic slices.\n",
      "The overall metric value is 30.652224858593335\n",
      "Using 2 as minimum support for determining problematic clusters.\n",
      "Using 1.7660831816225442 as minimum drop for determining problematic clusters.\n",
      "Identified 0 problematic slices.\n",
      "The overall metric value is 28.148714610514286\n",
      "Using 2 as minimum support for determining problematic clusters.\n",
      "Using 3.483197332267361 as minimum drop for determining problematic clusters.\n",
      "Identified 0 problematic slices.\n",
      "The overall metric value is 32.925277471550004\n",
      "Using 2 as minimum support for determining problematic clusters.\n",
      "Using 3.2452169309923686 as minimum drop for determining problematic clusters.\n",
      "Identified 0 problematic slices.\n",
      "Warning: Using hierarchical clustering failed. Probably the provided datapoints were not enough. HDBSCAN fallback might yield bad results!\n",
      "The overall metric value is 30.703774770099997\n",
      "Using 2 as minimum support for determining problematic clusters.\n",
      "Using 1.1272036900159357 as minimum drop for determining problematic clusters.\n",
      "Identified 0 problematic slices.\n"
     ]
    }
   ],
   "source": [
    "# Apply sliceguard cluster-by-cluster to find the most promising images per \"category\".\n",
    "# The intuition is that this mitigates some bias present in the CLIP Score metric.\n",
    "df[\"selection_group\"] = -1\n",
    "df[\"selection\"] = -1\n",
    "current_issue_idx = 0\n",
    "for cluster_idx in np.unique(clustering_partition):\n",
    "    cluster_embeddings = embeddings[clustering_partition==cluster_idx]\n",
    "    cluster_df = df[clustering_partition==cluster_idx]\n",
    "    \n",
    "    sg = SliceGuard()\n",
    "    issue_df = sg.find_issues(cluster_df, [\"clip_embedding\"],\n",
    "                   \"clip_score\",\n",
    "                   \"clip_score\",\n",
    "                   return_precomputed_metric,\n",
    "                   metric_mode=\"min\",\n",
    "                   min_support=2,\n",
    "                   min_drop=cluster_df[\"clip_score\"].std(),\n",
    "                  precomputed_embeddings={\"clip_embedding\": cluster_embeddings})\n",
    "    issue_identifiers = np.setdiff1d(issue_df[\"issue\"].unique(), [-1])\n",
    "    for issue_identifier in issue_identifiers:\n",
    "        issue_indices = issue_df[issue_df[\"issue\"] == issue_identifier].index\n",
    "        df.loc[issue_indices, \"selection_group\"] = cluster_idx\n",
    "        df.loc[issue_indices, \"selection\"] = current_issue_idx\n",
    "        current_issue_idx += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ef0505",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the results in Renumics Spotlight. The column \"selection\" contains the found clusters,\n",
    "# So just use it for browsing the results.\n",
    "spotlight.show(df, dtype={\"image\": Image, \"clip_embedding\": Embedding})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "73b3086a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the image clusters in a scatterplot on the 2D projection\n",
    "# This was used to create a visual for the blogpost. You don't necessarily need it.\n",
    "import math\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from PIL import Image\n",
    "groups = np.setdiff1d(df[\"selection_group\"].unique(), [-1])\n",
    "\n",
    "cluster_idx = 0\n",
    "for group in groups:\n",
    "    group_samples = df[df[\"selection_group\"] == group]\n",
    "    if len(group_samples) == 0:\n",
    "        continue\n",
    "    \n",
    "    clusters = np.setdiff1d(group_samples[\"selection\"].unique(), [-1])\n",
    "    for cluster in clusters:\n",
    "        \n",
    "        \n",
    "        cluster_samples = df[df[\"selection\"] == cluster]\n",
    "        \n",
    "        fig = px.scatter(df, x=\"projection_x\", y=\"projection_y\", color=\"clustering_1\")\n",
    "      \n",
    "        fig.update_layout(\n",
    "            showlegend=False,\n",
    "            coloraxis_showscale=False,\n",
    "            xaxis=dict(visible=False),\n",
    "            yaxis=dict(visible=False),\n",
    "        )\n",
    "        cluster_sample = 1\n",
    "        x_center = cluster_samples[\"projection_x\"].mean()\n",
    "        y_center = cluster_samples[\"projection_y\"].mean()\n",
    "        for _, row in cluster_samples.iterrows():\n",
    "            img = Image.open(row[\"image\"])\n",
    "            fig.add_layout_image(\n",
    "            x=x_center + 1.6 * math.cos(((2*math.pi) / len(cluster_samples)) * cluster_sample),\n",
    "            y=y_center + 1.6 * math.sin(((2*math.pi) / len(cluster_samples)) * cluster_sample),\n",
    "            source=img,\n",
    "            xref=\"x\",\n",
    "            yref=\"y\",\n",
    "            sizex=3,\n",
    "            sizey=3,\n",
    "            xanchor=\"center\",\n",
    "            yanchor=\"middle\",\n",
    "            )\n",
    "            cluster_sample += 1\n",
    "\n",
    "        fig.write_image(f\"slice_{cluster_idx:0>2}.png\", scale=2)\n",
    "#         fig.show()\n",
    "        cluster_idx += 1"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5ac22634",
   "metadata": {},
   "source": [
    "To create a gif:\n",
    "convert -delay 150 -loop 0 slice_*.png slices.gif"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
