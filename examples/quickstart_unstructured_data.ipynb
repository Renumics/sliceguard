{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Sliceguard â€“ Find critical data segments in your data (fast)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sliceguard is a python library for quickly finding **critical data slices** like outliers, errors, or biases. It works on **structured** and **unstructured** data.\n",
    "\n",
    "This notebook showcases especially the unstructured data case, so if you have tabular data instead have a look at [this notebook](examples/quickstart_structured_data.ipynb) instead\n",
    "\n",
    "It is interesting for you if you want to do the following:\n",
    "1. Find **performance issues** of your machine learning model.\n",
    "2. Find **anomalies and inconsistencies** in your data.\n",
    "3. Quickly **explore** your data using an interactive report to generate **insights**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run this notebook install and import sliceguard:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sliceguard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sliceguard import SliceGuard\n",
    "from sliceguard.data import from_huggingface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now download the demo dataset with our utility function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = from_huggingface(\"Matthijs/snacks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You now have the following dataframe containing an image column with a path to the raw image on the harddrive, a label and a split marker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df[df[\"label\"] == \"banana\"] # For this example we downsample the dataset. Remove to run on all data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check for Outliers and larger Error Groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature image was inferred as referring to raw data. If this is not the case, please specify in feature_types!\n",
      "Using default model for computing embeddings for feature image.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/daniel/code/sliceguard/.venv/lib/python3.10/site-packages/transformers/models/vit/feature_extraction_vit.py:28: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding computation on cuda with batch size 1 and multiprocessing None.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.weight', 'vit.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6a6856bd7a845edab132ed20200816f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/260 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-reducing feature image in mode outlier.\n",
      "Using op mix ratio 0.25.\n",
      "Using num dimensions 32.\n",
      "You didn't supply ground-truth labels and predictions. Will fit outlier detection model to find anomal slices instead.\n",
      "The overall metric value is 0.4712803953080694\n",
      "For outlier detection mode metric_mode will be set to min if not specified otherwise.\n",
      "Using 20 as maximum slice number to return.\n",
      "Using drop as sorting criterion for the slices to return.\n",
      "Identified 20 problematic slices.\n"
     ]
    }
   ],
   "source": [
    "sg = SliceGuard()\n",
    "issues = sg.find_issues(df[df[\"label\"] == \"popcorn\"], [\"image\"], drop_reference=\"overall\") # also try out drop_reference=\"parent\" for more class-specific results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_df = sg.report()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let sliceguard train a model to pinpoint problems even better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[48], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m sg \u001b[38;5;241m=\u001b[39m SliceGuard()\n\u001b[0;32m----> 2\u001b[0m issues \u001b[38;5;241m=\u001b[39m \u001b[43msg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_issues\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mimage\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlabel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdrop_reference\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moverall\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# also try out drop_reference=\"parent\" for more class-specific results\u001b[39;00m\n",
      "File \u001b[0;32m~/code/sliceguard/sliceguard/sliceguard.py:210\u001b[0m, in \u001b[0;36mSliceGuard.find_issues\u001b[0;34m(self, data, features, y, y_pred, metric, min_support, min_drop, n_slices, criterion, metric_mode, drop_reference, remove_outliers, feature_types, feature_orders, precomputed_embeddings, embedding_models, hf_auth_token, hf_num_proc, hf_batch_size, automl_task, automl_split_key, automl_train_split, automl_time_budget, automl_use_full_embeddings)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df \u001b[38;5;241m=\u001b[39m data  \u001b[38;5;66;03m# safe that here to not modify original dataframe accidentally\u001b[39;00m\n\u001b[1;32m    198\u001b[0m df \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mcopy()  \u001b[38;5;66;03m# assign to shorter name\u001b[39;00m\n\u001b[1;32m    200\u001b[0m (\n\u001b[1;32m    201\u001b[0m     feature_types,\n\u001b[1;32m    202\u001b[0m     encoded_data,\n\u001b[1;32m    203\u001b[0m     projection,\n\u001b[1;32m    204\u001b[0m     mfs,\n\u001b[1;32m    205\u001b[0m     clustering_df,\n\u001b[1;32m    206\u001b[0m     clustering_cols,\n\u001b[1;32m    207\u001b[0m     clustering_metric_cols,\n\u001b[1;32m    208\u001b[0m     prereduced_embeddings,\n\u001b[1;32m    209\u001b[0m     raw_embeddings,\n\u001b[0;32m--> 210\u001b[0m ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prepare_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    211\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    212\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    213\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    215\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    216\u001b[0m \u001b[43m    \u001b[49m\u001b[43mremove_outliers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    217\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeature_types\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_types\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    218\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeature_orders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_orders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    219\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprecomputed_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprecomputed_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    220\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_models\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membedding_models\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    221\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhf_auth_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_auth_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    222\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhf_num_proc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_num_proc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    223\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhf_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    224\u001b[0m \u001b[43m    \u001b[49m\u001b[43mautoml_split_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mautoml_split_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[43m    \u001b[49m\u001b[43mautoml_train_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mautoml_train_split\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    226\u001b[0m \u001b[43m    \u001b[49m\u001b[43mautoml_task\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mautoml_task\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    227\u001b[0m \u001b[43m    \u001b[49m\u001b[43mautoml_time_budget\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mautoml_time_budget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    228\u001b[0m \u001b[43m    \u001b[49m\u001b[43mautoml_use_full_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mautoml_use_full_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    229\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    231\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mfs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    232\u001b[0m     overall_metric \u001b[38;5;241m=\u001b[39m mfs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39moverall\u001b[38;5;241m.\u001b[39mvalues[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/code/sliceguard/sliceguard/sliceguard.py:491\u001b[0m, in \u001b[0;36mSliceGuard._prepare_data\u001b[0;34m(self, data, features, y, y_pred, metric, remove_outliers, feature_types, feature_orders, precomputed_embeddings, embedding_models, hf_auth_token, hf_num_proc, hf_batch_size, automl_task, automl_split_key, automl_train_split, automl_time_budget, automl_use_full_embeddings)\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_prepare_data\u001b[39m(\n\u001b[1;32m    459\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    460\u001b[0m     data: pd\u001b[38;5;241m.\u001b[39mDataFrame,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    479\u001b[0m     automl_use_full_embeddings\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    480\u001b[0m ):\n\u001b[1;32m    481\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[1;32m    482\u001b[0m         \u001b[38;5;28mall\u001b[39m([(f \u001b[38;5;129;01min\u001b[39;00m data\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;129;01mor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m precomputed_embeddings) \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m features])\n\u001b[1;32m    483\u001b[0m     ) \u001b[38;5;129;01mand\u001b[39;00m (\n\u001b[1;32m    484\u001b[0m         (\n\u001b[1;32m    485\u001b[0m             (y_pred \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m (y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m (metric \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    486\u001b[0m         )  \u001b[38;5;66;03m# Completly supervised case\u001b[39;00m\n\u001b[1;32m    487\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m    488\u001b[0m             (y_pred \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m (y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m (metric \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    489\u001b[0m         )  \u001b[38;5;66;03m# fit own model\u001b[39;00m\n\u001b[1;32m    490\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m--> 491\u001b[0m             (y_pred \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m (y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m (metric \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    492\u001b[0m         )  \u001b[38;5;66;03m# Completely unsupervised case (outlier based)\u001b[39;00m\n\u001b[1;32m    493\u001b[0m     )\n\u001b[1;32m    495\u001b[0m     df \u001b[38;5;241m=\u001b[39m data  \u001b[38;5;66;03m# just rename the variable for shorter naming\u001b[39;00m\n\u001b[1;32m    497\u001b[0m     \u001b[38;5;66;03m# Try to infer the column dtypes\u001b[39;00m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "sg = SliceGuard()\n",
    "issues = sg.find_issues(df.sample(1000), [\"image\"], y=\"label\", drop_reference=\"overall\") # also try out drop_reference=\"parent\" for more class-specific results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train an own advanced model and find its weaknesses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
